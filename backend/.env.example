# Local inference backend configuration
PORT=3001
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gpt-oss:20b

# Optional: Fallback to cloud APIs if local fails
OPENAI_API_KEY=your_openai_key_here
CLAUDE_API_KEY=your_claude_key_here